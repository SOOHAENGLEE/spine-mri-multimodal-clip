# src/models/generator.py
import torch
import torch.nn as nn
from transformers import BartForConditionalGeneration
from transformers.modeling_outputs import BaseModelOutput # ğŸ”¥ [í•µì‹¬] í‘œì¤€ ì¶œë ¥ í´ë˜ìŠ¤ ì‚¬ìš©
from torchvision.models import convnext_base

class ImageEncoder25D(nn.Module):
    def __init__(self, embed_dim=768, in_channels=15):
        super().__init__()
        # ConvNeXt Base ë¡œë“œ
        try: backbone = convnext_base(weights="IMAGENET1K_V1")
        except: backbone = convnext_base(pretrained=True)
        
        # ì²« ë ˆì´ì–´ ì±„ë„ ìˆ˜ì • (3 -> 15)
        old_conv = backbone.features[0][0]
        new_conv = nn.Conv2d(in_channels, old_conv.out_channels, 
                             kernel_size=old_conv.kernel_size, stride=old_conv.stride, 
                             padding=old_conv.padding)
        # ê°€ì¤‘ì¹˜ ë³µì‚¬ (í‰ê· )
        with torch.no_grad():
            new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True).repeat(1, in_channels, 1, 1)
            
        backbone.features[0][0] = new_conv
        self.backbone = backbone.features
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.proj = nn.Linear(1024, embed_dim) # ConvNeXt Base output=1024

    def forward(self, x):
        x = self.backbone(x) # (B, 1024, H, W)
        x = self.pool(x).flatten(1) # (B, 1024)
        return self.proj(x) # (B, embed_dim)

class MRIReportGenerator(nn.Module):
    def __init__(self, text_model_name="GanjinZero/biobart-base"):
        super().__init__()
        self.bart = BartForConditionalGeneration.from_pretrained(text_model_name)
        # Note: BioBART d_model is 768.
        self.img_encoder = ImageEncoder25D(embed_dim=self.bart.config.d_model, in_channels=15)

    def forward(self, images, labels=None):
        img_embeds = self.img_encoder(images).unsqueeze(1) # (B, 1, Hidden)
        
        if labels is not None:
            # í•™ìŠµ ëª¨ë“œ: encoder_outputsë¥¼ BaseModelOutputìœ¼ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
            encoder_outputs = BaseModelOutput(last_hidden_state=img_embeds)
            return self.bart(inputs_embeds=None, encoder_outputs=encoder_outputs, labels=labels)
        else:
            # ì¶”ë¡  ëª¨ë“œ
            return self.generate(images, max_length=128)

    # ğŸ”¥ [ìˆ˜ì •] generate ë©”ì„œë“œ: í‘œì¤€ BaseModelOutput ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
    def generate(self, images, **kwargs):
        img_embeds = self.img_encoder(images).unsqueeze(1)
        
        # Hugging Faceê°€ ê¸°ëŒ€í•˜ëŠ” í‘œì¤€ ê°ì²´ë¡œ ë³€í™˜
        encoder_outputs = BaseModelOutput(last_hidden_state=img_embeds)
        
        return self.bart.generate(encoder_outputs=encoder_outputs, **kwargs)