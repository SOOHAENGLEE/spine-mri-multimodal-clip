# scripts/03_gen_pseudo_labels.py (ìˆ˜ì •ëœ íŒŒì¼)
import sys
import os
import torch
import json
import argparse
import yaml
import math
from transformers import AutoTokenizer
from tqdm import tqdm
from accelerate import Accelerator
from pathlib import Path # Pathlib ì¶”ê°€
import shutil # shutil ì¶”ê°€ (ë””ë ‰í† ë¦¬ ì‚­ì œìš©)

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(current_dir))

from src.models.summarizer import ReportSummarizer

def main(args):
    # 1. Config & Accelerator
    with open(args.config, 'r') as f:
        cfg = yaml.safe_load(f)

    # Accelerate ì´ˆê¸°í™” (Mixed Precision ì ìš©)
    accelerator = Accelerator(mixed_precision="fp16")
    device = accelerator.device

    # ê²½ë¡œ ì„¤ì •
    OUTPUT_BASE_DIR = Path(cfg['paths']['output_dir'])
    TEMP_DIR_NAME = "temp_pseudo_parts" # ì„ì‹œ íŒŒì¼ë“¤ì„ ëª¨ì„ ë””ë ‰í† ë¦¬ ì´ë¦„
    TEMP_PARTS_DIR = OUTPUT_BASE_DIR / TEMP_DIR_NAME
    
    MODEL_PATH = f"{cfg['paths']['result_dir']}/summarizer/best_model.pt"
    UNLABELED_PATH = OUTPUT_BASE_DIR / "unlabeled.json"
    
    # ê° GPU í”„ë¡œì„¸ìŠ¤ê°€ ì €ì¥í•  ì„ì‹œ íŒŒì¼ ê²½ë¡œ (TEMP_PARTS_DIR ì•„ë˜ì— ì €ì¥)
    TEMP_OUTPUT_PATH = TEMP_PARTS_DIR / f"aug_part_{accelerator.process_index}.json"
    FINAL_OUTPUT_PATH = OUTPUT_BASE_DIR / "augmented.json"
    LABELED_PATH = OUTPUT_BASE_DIR / "labeled.json"
    
    MODEL_NAME = cfg['models']['summarizer']['name']
    NUM_RETURN_SEQUENCES = 3
    BATCH_SIZE = 16 # GPUë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ

    if accelerator.is_main_process:
        print(f"ğŸš€ Generating Pseudo-labels with {accelerator.num_processes} GPUs")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        TEMP_PARTS_DIR.mkdir(parents=True, exist_ok=True)
        print(f"   Created temporary directory for parts: {TEMP_PARTS_DIR}")
        
        if not os.path.exists(UNLABELED_PATH):
            print(f"âŒ {UNLABELED_PATH} not found.")
            return

    # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„ì‹œ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    accelerator.wait_for_everyone()
    
    # 2. ë°ì´í„° ë¡œë“œ ë° ë¶„í•  (Sharding)
    with open(UNLABELED_PATH, 'r') as f:
        all_unlabeled = json.load(f)
    
    # ì „ì²´ ë°ì´í„°ë¥¼ GPU ê°œìˆ˜ë§Œí¼ ë‚˜ëˆ”
    total_size = len(all_unlabeled)
    chunk_size = math.ceil(total_size / accelerator.num_processes)
    start_idx = accelerator.process_index * chunk_size
    end_idx = min(start_idx + chunk_size, total_size)
    
    my_chunk = all_unlabeled[start_idx:end_idx]
    
    print(f"   [GPU {accelerator.process_index}] Processing {len(my_chunk)} samples ({start_idx}~{end_idx})")

    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = ReportSummarizer(MODEL_NAME)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ (Map location ì£¼ì˜)
    if os.path.exists(MODEL_PATH):
        # DDP í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ëŠ” 'module.' prefixê°€ ìˆì„ ìˆ˜ ìˆìŒ
        state_dict = torch.load(MODEL_PATH, map_location='cpu')
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        model.load_state_dict(new_state_dict)
    
    # ëª¨ë¸ì„ Acceleratorë¡œ ì¤€ë¹„
    model = model.to(device)
    model.eval()

    # 4. ì¶”ë¡  (Batch Processing)
    augmented_data = []
    
    # ë‚´ ì²­í¬ ì•ˆì—ì„œ ë°°ì¹˜ ì²˜ë¦¬
    for i in tqdm(range(0, len(my_chunk), BATCH_SIZE), desc=f"GPU {accelerator.process_index}", disable=not accelerator.is_local_main_process):
        batch_items = my_chunk[i : i+BATCH_SIZE]
        texts = [item['full_report'] for item in batch_items]
        
        inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
        
        with torch.no_grad():
            summary_ids = model.generate(
                inputs.input_ids, 
                attention_mask=inputs.attention_mask, 
                max_length=128,
                num_return_sequences=NUM_RETURN_SEQUENCES,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                no_repeat_ngram_size=3
            )
            
        summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)
        
        # ê²°ê³¼ ë§¤í•‘
        for j, item in enumerate(batch_items):
            current_sums = summaries[j*NUM_RETURN_SEQUENCES : (j+1)*NUM_RETURN_SEQUENCES]
            for k, summ in enumerate(current_sums):
                new_item = item.copy()
                new_item['id'] = f"{item['id']}_aug{k}"
                new_item['summary'] = summ
                augmented_data.append(new_item)

    # 5. ë¶€ë¶„ ê²°ê³¼ ì €ì¥
    with open(TEMP_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(augmented_data, f, indent=2, ensure_ascii=False)
    
    # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ íŒŒì¼ì„ ì“¸ ë•Œê¹Œì§€ ëŒ€ê¸°
    accelerator.wait_for_everyone()

    # 6. ê²°ê³¼ ë³‘í•© ë° ì •ë¦¬ (Main Process Only)
    if accelerator.is_main_process:
        print("ğŸ”„ Merging results from all GPUs...")
        final_augmented = []
        for i in range(accelerator.num_processes):
            part_path = TEMP_PARTS_DIR / f"aug_part_{i}.json"
            if os.path.exists(part_path):
                with open(part_path, 'r') as f:
                    final_augmented.extend(json.load(f))
        
        # Labeled ë°ì´í„°ì™€ í•©ì¹˜ê¸°
        if os.path.exists(LABELED_PATH):
            with open(LABELED_PATH, 'r') as f:
                labeled_data = json.load(f)
            final_dataset = labeled_data + final_augmented
        else:
            final_dataset = final_augmented
            
        with open(FINAL_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… Created {FINAL_OUTPUT_PATH} with {len(final_dataset)} samples.")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì „ì²´ ì‚­ì œ (ì§¤ì§¤ì´ íŒŒì¼ ì •ë¦¬)
        try:
            shutil.rmtree(TEMP_PARTS_DIR)
            print(f"ğŸ§¹ Cleaned up temporary directory: {TEMP_PARTS_DIR}")
        except OSError as e:
            print(f"âš ï¸ Error removing temporary directory {TEMP_PARTS_DIR}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/model_config.yaml")
    args = parser.parse_args()
    main(args)